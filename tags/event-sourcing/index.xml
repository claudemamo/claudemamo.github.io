<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Event Sourcing on On Code &amp; Design</title>
    <link>http://www.oncodesign.io/tags/event-sourcing/</link>
    <description>Recent content in Event Sourcing on On Code &amp; Design</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Dec 2013 21:01:00 +0000</lastBuildDate>
    <atom:link href="http://www.oncodesign.io/tags/event-sourcing/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Apache Kafka for Event Sourcing</title>
      <link>http://www.oncodesign.io/2013/12/11/apache-kafka-for-event-sourcing/</link>
      <pubDate>Wed, 11 Dec 2013 21:01:00 +0000</pubDate>
      
      <guid>http://www.oncodesign.io/2013/12/11/apache-kafka-for-event-sourcing/</guid>
      <description>&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;a href=&#34;http://martinfowler.com/eaaDev/EventSourcing.html&#34; target=&#34;_blank&#34;&gt;Event Sourcing&lt;/a&gt; is a pattern intended for &#34;&lt;i&gt;capturing all changes to an application state as a sequence of events&lt;/i&gt;&#34;. As explained by Fowler, the pattern is useful when you want the ability to completely rebuild the application state, perform temporal querying, or replay events. The &lt;a href=&#34;http://www.infoq.com/presentations/LMAX&#34; target=&#34;_blank&#34;&gt;LMAX platform&lt;/a&gt; is a famous example where Event Sourcing is applied to keep all application state in-memory and consequently contributing to the system&#39;s surprisingly high throughput and low latency.&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;While investigating the architectural components of &lt;a href=&#34;http://samza.incubator.apache.org/&#34; target=&#34;_blank&#34;&gt;Samza&lt;/a&gt;, I came across a component that can be of great help when implementing Event Sourcing:&amp;nbsp;&lt;a href=&#34;https://kafka.apache.org/&#34; target=&#34;_blank&#34;&gt;Apache Kafka&lt;/a&gt;. &lt;a href=&#34;http://research.microsoft.com/en-us/um/people/srikanth/netdb11/netdb11papers/netdb11-final12.pdf&#34; target=&#34;_blank&#34;&gt;Created&lt;/a&gt; by the folks at LinkedIn as a solution to their log processing requirements, Kafka is a broker with message replay built-in.&lt;/div&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&#34;text-align: left;&#34;&gt;Kafka consumers receive messages from publish/subscribe channels known as topics. A topic is divided into user-defined partitions where a partition can serve messages only to a single consumer process. Balancing the message load between consumers is a matter of adding more partitions to the topic, assigning those partitions to other consumer instances, and finally, publishing messages to all topic partitions in a round robin fashion.&lt;/div&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-kvjAcN0vFfg/Uqb4m63H3_I/AAAAAAAAAGw/zJ7XrGdjiDA/s1600/kafka-concepts.png&#34; imageanchor=&#34;1&#34; style=&#34;clear: left; float: left; margin-bottom: 1em; margin-right: 1em; text-align: left;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-kvjAcN0vFfg/Uqb4m63H3_I/AAAAAAAAAGw/zJ7XrGdjiDA/s640/kafka-concepts.png&#34; height=&#34;356&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;What fascinates about Kafka is that at any point in time a consumer can rewind back through the history of messages and re-consume messages at a particular offset. In the above diagram, Consumer B can consume the latest messages or replay messages, say, starting from offset 1.&amp;nbsp;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;At face value we could be forgiven to think that a broker with in-built message replay would have trouble achieving high throughput for large message volumes. After all, Kafka is retaining&amp;nbsp;unconsumed&amp;nbsp;as well as consumed messages on disk: presumably costlier than simply keeping unconsumed messages in memory. However, a &lt;a href=&#34;http://kafka.apache.org/documentation.html#design&#34; target=&#34;_blank&#34;&gt;few&lt;/a&gt; clever design decisions, such as relying on the OS page cache and minimising random disk I/O, gave LinkedIn engineers impressive throughput results when comparing Kafka against both ActiveMQ and RabbitMQ.&lt;/div&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;With the basic concepts and performance considerations out of the way, let me illustrate my point about Kafka&#39;s suitability for Event Sourcing by giving a &lt;a href=&#34;https://github.com/claudemamo/kafka-replays&#34; target=&#34;_blank&#34;&gt;code example&lt;/a&gt;:&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/7840143.js?file=MyProducer.java&#34;&gt;&lt;/script&gt;The above producer publishes, for a number of times, a message to the topic &lt;i&gt;ossandme &lt;/i&gt;on partition 0. In particular, it creates a message by instantiating the&amp;nbsp;&lt;i&gt;KeyedMessage&lt;/i&gt; class with the following parameters (line 19):&lt;/div&gt;&lt;/div&gt;&lt;ul&gt;&lt;li style=&#34;text-align: left;&#34;&gt;Name of the topic to which the message is published.&lt;/li&gt;&lt;li style=&#34;text-align: left;&#34;&gt;ID of the partition the message will sit on.&lt;/li&gt;&lt;li style=&#34;text-align: left;&#34;&gt;Message content, in this case, the time the message was published.&lt;/li&gt;&lt;/ul&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;The following consumer pulls messages from the&amp;nbsp;topic&amp;nbsp;&lt;i&gt;ossandme&lt;/i&gt;:&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/7840143.js?file=MyConsumer.java&#34;&gt;&lt;/script&gt;For each message received, the application outputs the message&#39;s offset on partition 0 in addition to its content (line 50). The first thing to observe is that I&#39;ve programmed against Kafka&#39;s low-level&amp;nbsp;&lt;a href=&#34;http://kafka.apache.org/documentation.html#simpleconsumerapi&#34; target=&#34;_blank&#34;&gt;SimpleConsumer&lt;/a&gt;&amp;nbsp;API. Alternatively, I could have opted for the&amp;nbsp;&lt;a href=&#34;http://kafka.apache.org/documentation.html#highlevelconsumerapi&#34; target=&#34;_blank&#34;&gt;High Level Consumer&lt;/a&gt;&amp;nbsp;API to reduce development time. I chose the former because with the latter I was unable to find a way to replay any set of messages I wanted.&amp;nbsp;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;Event Sourcing comes into play when an exception occurs. On exception, the application rewinds back to the first message in the partition and re-attempts to process all of the partition&#39;s messages (line 24). I like the fact that, to get this type of behaviour, I didn&#39;t have to introduce a database but simply leveraged the broker&#39;s message replay capability. Without a database, I&#39;ve one less moving part to think about in my architecture.&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;Kafka is a young project and I&#39;m interested to see how it matures. I&#39;m keen to hear people&#39;s experiences using Kafka and whether it proved to be the right solution for them. As the project matures, I suspect we&#39;ll hear more often about Kafka in our technical discussions along with the other, more established, open source brokers.&lt;/div&gt;&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>