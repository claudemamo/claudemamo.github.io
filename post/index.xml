<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on On Code &amp; Design</title>
    <link>http://oncodesign.io/post/</link>
    <description>Recent content in Posts on On Code &amp; Design</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Dec 2015 11:23:00 +0000</lastBuildDate>
    <atom:link href="http://oncodesign.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Messaging for ETL Anti-Pattern</title>
      <link>http://oncodesign.io/2015/12/09/messaging-for-etl-anti-pattern/</link>
      <pubDate>Wed, 09 Dec 2015 11:23:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2015/12/09/messaging-for-etl-anti-pattern/</guid>
      <description>&lt;p&gt;This is a trap I&amp;rsquo;ve observed numerous professionals in the software industry fall into. After all, quite a few people I talk to like to think of messaging as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_the_instrument&#34; target=&#34;_blank&#34;&gt;golden hammer&lt;/a&gt;. The sales folks surely want us to believe that this is the case. So many organisations have dug themselves into a hole by using &lt;i&gt;Messaging for ETL&lt;/i&gt; that I&amp;rsquo;m classifying this problem as an anti-pattern and giving it a brief overview.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Context&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;The business mandates end-of-day reports. The data required for these reports is locked up in CSV files hosted on a FTP server. Each file can range from hundreds to thousands of MBs (i.e., GBs) of records. Records need to be cleansed, massaged, enriched, and transformed from one format to another. Furthermore, some record sets need to be joined with others. At the final stages of the process, the target records have to be written to files and uploaded to a CRM.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Problem&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;The business decides to use&amp;nbsp;&lt;i&gt;Messaging for ETL&lt;/i&gt;. The rationality behind such a decision can vary. One argument might be that some messaging solutions are suited for ETL tasks because they come with a broad set of protocol adaptors and have sophisticated transformation capabilities. The messaging solution could be an ESB, even though the term appears to have fallen out of fashion with the marketing crowd nowadays.&lt;br /&gt;&lt;br /&gt;Predictably, the development team models each record as a message. &lt;a href=&#34;http://www.enterpriseintegrationpatterns.com/patterns/messaging/index.html&#34; target=&#34;_blank&#34;&gt;Messaging patterns&lt;/a&gt; are used to solve common recurring problems. For example,&amp;nbsp;&lt;a href=&#34;http://www.enterpriseintegrationpatterns.com/patterns/messaging/MessageChannel.html&#34; target=&#34;_blank&#34;&gt;message queues&lt;/a&gt;&amp;nbsp;in order to process the records concurrently between&amp;nbsp;&lt;a href=&#34;http://www.enterpriseintegrationpatterns.com/patterns/messaging/CompetingConsumers.html&#34; target=&#34;_blank&#34;&gt;competing consumers&lt;/a&gt;;&amp;nbsp;&lt;a href=&#34;http://www.enterpriseintegrationpatterns.com/patterns/messaging/MessageTranslator.html&#34; target=&#34;_blank&#34;&gt;message translators&lt;/a&gt; for cleansing, massaging, enriching and transforming the data; &lt;a href=&#34;http://www.enterpriseintegrationpatterns.com/patterns/messaging/Aggregator.html&#34; target=&#34;_blank&#34;&gt;aggregator&amp;nbsp;&lt;/a&gt;to join records. Applying these patterns is sufficiently easy if the messaging solution has them baked-in.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-40aObn1aqgY/VmcTq4ts1sI/AAAAAAAAANM/xJS-1IWlygU/s1600/messaging-for-etl.jpg&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;356&#34; src=&#34;http://1.bp.blogspot.com/-40aObn1aqgY/VmcTq4ts1sI/AAAAAAAAANM/xJS-1IWlygU/s400/messaging-for-etl.jpg&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;b&gt;Consequences&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;&lt;div style=&#34;text-align: left;&#34;&gt;Loosely speaking, the primitives offered by messaging solutions are &lt;b&gt;overly&amp;nbsp;low-level and general&lt;/b&gt; for ETL operations. Taking the context above, reasoning about the application becomes hard when you have more than a handful of joins. Aggregators think in terms of correlation keys while we tend to think in higher terms of join columns. Similarly, message queueing and competing consumers is a low-level way of concurrently processing records. It&amp;rsquo;s more useful for us to think in terms of&amp;nbsp;&lt;i&gt;partitioning&lt;/i&gt;&amp;nbsp;the&amp;nbsp;&lt;i&gt;record stream&lt;/i&gt;&amp;nbsp;in order to achieve concurrency and not having to worry about queues, consumers, and so on.&lt;/div&gt;&lt;br /&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cognitive_dissonance&#34; target=&#34;_blank&#34;&gt;Conceptual dissonance&lt;/a&gt; is one aspect to the problem of&amp;nbsp;&lt;i&gt;Messaging for ETL&lt;/i&gt;. Another aspect is performance. Treating each record as a packet of data and processing them in a single go leads to a high rate of message traffic that is &lt;i&gt;uniform&lt;/i&gt; over time. From my experience, this often causes a significant, if not drastic, drop in throughput simply because most messaging solutions can&amp;rsquo;t &lt;i&gt;reliably&lt;/i&gt; cope with this pattern of traffic. Lock contention is a key factor for this. To illustrate the point, consider the message ID. Several messaging solutions generate a UUID, representing the message ID, and add it to the message before going on to publishing it. Generating a UUID involves obtaining a global lock. As the reader produces hundreds of thousands of messages while it&amp;rsquo;s churning through the CSV files, concurrently, the aggregator is combining individual messages to produce messages with new UUIDs. Given the stream of messages is constant and without any respite, the result is a high rate of lock contention caused by the reader and aggregator fighting each other out for the lock to generate UUIDs.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Refactored solution&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;One way to untangle this anti-pattern is to migrate the data intensive logic to another tool. A staging database may be a good initial candidate where you can leverage SQL for the heavy-lifting. Other candidates include ones specifically built for ETL. This doesn&amp;rsquo;t mean you&amp;rsquo;re stuck with having to purchase a proprietary ETL tool. Open-source alternatives do exist like &lt;a href=&#34;http://community.pentaho.com/&#34; target=&#34;_blank&#34;&gt;Pentaho&lt;/a&gt;. If the data you&amp;rsquo;re transforming is in the realm of &amp;ldquo;Big Data&amp;rdquo;, where you need to distribute its processing across a cluster of nodes, map/reduce frameworks such as Apache Spark or Apache Hadoop should be considered.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Implementing a Replicated Token Service with JSON Web Tokens</title>
      <link>http://oncodesign.io/2015/11/09/implementing-a-replicated-token-service-with-json-web-tokens/</link>
      <pubDate>Mon, 09 Nov 2015 13:42:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2015/11/09/implementing-a-replicated-token-service-with-json-web-tokens/</guid>
      <description>&lt;div&gt;Last week I observed one of the 8 fallacies of distributed systems in action:&lt;br /&gt;&lt;blockquote class=&#34;tr_bq&#34;&gt;&lt;blockquote class=&#34;tr_bq&#34;&gt;&lt;i&gt;&#34;Topology doesn&#39;t change&#34;&lt;/i&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;&lt;/div&gt;&lt;div&gt;A client of mine deployed the latest versions of his web services to a highly-available QA environment. Sanity tests gave initial confirmation that the system was behaving as expected. But then, the QA team reported weird behaviour in the system&#39;s offline functionality. So I was called in the figure out the problem. The logs showed an application getting random HTTP 401s from the system&#39;s token service.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;This token service is a Java web application that creates and verifies &lt;a href=&#34;http://jwt.io/&#34; target=&#34;_blank&#34;&gt;JSON Web Tokens&lt;/a&gt; (JWTs). A client receives a 200 HTTP OK from the service for a token passing verification. Otherwise, it receives a 401 HTTP Unauthorized code. On startup, the token service creates a public/private key pair (PPK) &amp;nbsp;in-memory&amp;nbsp;for signing and verifying these tokens. I knew the token service in QA was replicated and requests to replicas were load-balanced in a round-robin fashion. This quickly led me to the realisation that the issue occurred when (1) a replica of the token service verified a token with its own public key and (2) the same token was created as well signed by a different replica with its own private key. This issue wasn&#39;t caught in the developer&#39;s testing environment because services weren&#39;t replicated.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;I&#39;m going to describe a solution I implemented for this problem because, though it&#39;s simple to program, such a solution might not be obvious. All shown code is in Java or SQL but it should be relatively easy to adapt the code to the technologies of your choice.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;At an abstract level, the solution is to have each token service replica&#39;s public key and key ID visible to all other replicas. In addition to making the key ID visible to the set of replicas, the signer embeds the key ID in the created token before signing it with its own private key. This allows the verifier to know which public key to use for verifying the token. When the token service accepts a request to verify a token, it extracts the key ID from the token to lookup the public key to use for verifying it. Security-wise, this approach enables us to keep the private key secret with respect to the other token service replicas.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Now let&#39;s delve into the details. Given that the token service replicas share a database, I re-use the database to share the public keys and key IDs between replicas. In a relational database context, the schema for holding such information might look like this:&lt;/div&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/37732487df6f59fc7177.js?file=schema.sql&#34;&gt;&lt;/script&gt;&lt;ol&gt;&lt;li&gt;&lt;i&gt;nodeId&lt;/i&gt; is a UUID representing the replica owning the table row. This enables me to delete&amp;nbsp;the row owned by a replica when it gracefully shuts down, reducing, but not eliminating, the likelihood of orphan records.&lt;/li&gt;&lt;br /&gt;&lt;li&gt;&lt;i&gt;name&lt;/i&gt; identifies the type of configuration. Although in this solution I&#39;m only storing the public key in the table, you might want to store other configurations.&lt;/li&gt;&lt;br /&gt;&lt;li&gt;&lt;i&gt;value_&lt;/i&gt; is where I store the actual public key along with the key ID.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;In the token service, I use&amp;nbsp;&lt;a href=&#34;https://bitbucket.org/b_c/jose4j/wiki/Home&#34; target=&#34;_blank&#34;&gt;jose.4.j&lt;/a&gt;&amp;nbsp;0.4.4, an open-source Java implementation of JWT, for generating and verifying tokens. Before I can go on to generate/verify a token, first I need create a PPK and register the public key, including its key ID, so that it can be read by other replicas:&lt;/div&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/37732487df6f59fc7177.js?file=Listener(1).java&#34;&gt;&lt;/script&gt;The above code is executed at startup and merits a brief explanation:&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;Line 7-8&lt;i&gt;: RsaJwkGenerator.generateJwk(2048)&lt;/i&gt; returns a 2048-bit PPK. The key ID for the PPK is set to the node ID which is simply a UUID created as well at startup.&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;Line 9:&amp;nbsp;&lt;i&gt;ConfigurationDataMapper.insertConfiguration(...)&lt;/i&gt; registers the public key by adding a record to the database table &lt;i&gt;Configuration&lt;/i&gt;. Its parameters map to the table columns&amp;nbsp;&lt;i&gt;nodeId&lt;/i&gt;, &lt;i&gt;name&lt;/i&gt;, and &lt;i&gt;value_,&lt;/i&gt;&amp;nbsp;respectively.&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;Line 9:&amp;nbsp;&lt;i&gt;rsaJsonWebKey.toJson() &lt;/i&gt;does the job of serialising the public key and key ID to JSON for us. Note the&amp;nbsp;&lt;i&gt;toJson()&lt;/i&gt;&amp;nbsp;method does NOT include the private key in the returned JSON.&lt;br /&gt;&lt;br /&gt;Line 10: Finally, the PPK is saved in the service&#39;s context in order to read the private key later on for signing the token.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;As mentioned above, the token service creates signed tokens for clients. The code for this is implemented in the &lt;i&gt;createToken(...)&lt;/i&gt; method:&lt;/div&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/37732487df6f59fc7177.js?file=TokenFactory.java&#34;&gt;&lt;/script&gt;I blatantly copied the code from&amp;nbsp;jose.4.j&#39;s excellent &lt;a href=&#34;https://bitbucket.org/b_c/jose4j/wiki/JWT%20Examples&#34; target=&#34;_blank&#34;&gt;examples page&lt;/a&gt; from where you can find an explanation of what it does.&amp;nbsp;However, I want to highlight that I&#39;m passing the PPK I saved earlier in the service context to &lt;i&gt;createToken(...)&lt;/i&gt;. Additionally, observe that on line 16 I&#39;m setting the token&#39;s key ID to the PPK&#39;s key ID which is the node ID.&lt;br /&gt;&lt;br /&gt;&lt;div&gt;On receiving a request to verify a token, the service fetches all registered public keys and key IDs from the database before verifying the token [1]:&lt;/div&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/37732487df6f59fc7177.js?file=TokenService(1).java&#34;&gt;&lt;/script&gt;&lt;div&gt;In the above method, the public keys are (1) re-constructed from the JSON persisted to the database and (2) added to a list. The list is passed to the &lt;i&gt;isValid(...)&lt;/i&gt; method along with the token. i&lt;i&gt;sValid(...)&lt;/i&gt;&amp;nbsp;returns true if a token passes verification, otherwise false:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/37732487df6f59fc7177.js?file=TokenService(2).java&#34;&gt;&lt;/script&gt;In &lt;i&gt;isValid(...)&lt;/i&gt;, I pass the list of public keys to the&amp;nbsp;&lt;i&gt;JwksVerificationKeyResolver&lt;/i&gt;&amp;nbsp;class constructor to create an object that resolves the public key to use for verifying the token according to the key ID extracted from the received token. The rest of the code builds a&amp;nbsp;&lt;i&gt;JwtConsumer&lt;/i&gt;&amp;nbsp;object to verify the token.&lt;br /&gt;&lt;br /&gt;The last item to tackle is to have a token service replica, that is shutting down gracefully, delete its public key from the configuration table:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/37732487df6f59fc7177.js?file=Listener(2).java&#34;&gt;&lt;/script&gt;This is required because the replica&#39;s private key and node ID are kept in-memory and therefore lost on shutdown. Of course, this isn&#39;t a foolproof way of eliminating orphan records. Furthermore, it&#39;s possible that a token signed by a replica is still in circulation after the replica has shutdown causing the token to fail verification. I&#39;ll leave these problems as exercises for the reader to solve.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;span class=&#34;num&#34; style=&#34;font-family: &amp;quot;times&amp;quot; , &amp;quot;times new roman&amp;quot; , serif;&#34;&gt;1: &lt;i&gt;createToken(...)&lt;/i&gt; definitely has room for improvement in terms of performance.&amp;nbsp;&lt;/span&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Describing API Key Authentication in RAML</title>
      <link>http://oncodesign.io/2015/09/25/describing-api-key-authentication-in-raml/</link>
      <pubDate>Fri, 25 Sep 2015 14:56:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2015/09/25/describing-api-key-authentication-in-raml/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve finally figured out how to say in &lt;a href=&#34;http://raml.org/&#34;&gt;RAML&lt;/a&gt; that API operations are protected by an API key query parameter:&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/045dcd97953b69fdad6c.js&#34;&gt;&lt;/script&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Retiring Kafka Web Console</title>
      <link>http://oncodesign.io/2015/09/19/retiring-kafka-web-console/</link>
      <pubDate>Sat, 19 Sep 2015 13:31:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2015/09/19/retiring-kafka-web-console/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been a busy bee the past few months. The lack of activity on my blog and GitHub is a testament to this. Given my current priorities, I&amp;rsquo;ve taken the decision to retire &lt;a href=&#34;https://github.com/claudemamo/kafka-web-console&#34; target=&#34;_blank&#34;&gt;Kafka Web Console&lt;/a&gt;. Don&amp;rsquo;t despair yourself! &lt;a href=&#34;https://github.com/yahoo/kafka-manager&#34; target=&#34;_blank&#34;&gt;Kafka Manager&lt;/a&gt; appears to be a more sophisticated alternative to what I&amp;rsquo;ve developed, and besides, it&amp;rsquo;s maintained by Yahoo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamically Create Rules using Drools &amp; Rule Templates</title>
      <link>http://oncodesign.io/2015/08/10/dynamically-create-rules-using-drools--rule-templates/</link>
      <pubDate>Mon, 10 Aug 2015 15:12:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2015/08/10/dynamically-create-rules-using-drools--rule-templates/</guid>
      <description>&lt;p&gt;Rules are used for a variety of stuff in the systems we build. Most often these rules are hard-coded in our application logic. The trouble is that sometimes we want to have the end-user the ability to define his own rules. Imagine an order processing system. The supplier wants to be notified on any range of events as they occur throughout the system but the notification rules are not known ahead of time. Such a rule could be for a late payment or a highly lucrative order event. In Java, the latter rule can be modelled as follows [1]:&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/50736b6ff70f75ae8bd7.js?file=Program(1).java&#34;&gt;&lt;/script&gt;Supporting conjoined conditions in a rule requires us that we tweak the previous example:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/50736b6ff70f75ae8bd7.js?file=Program(2).java&#34;&gt;&lt;/script&gt;I consider it a risky proposition to write your own primitive rules engine to evaluate rules like the above. I much prefer a solution leveraging Drools 6 in combination with &lt;b&gt;&lt;a href=&#34;https://docs.jboss.org/drools/release/6.2.0.Final/drools-docs/html_single/#d0e5627&#34; target=&#34;_blank&#34;&gt;Rule Templates&lt;/a&gt;&lt;/b&gt;. Rule Templates is an awesome Drools feature giving you the ability to define abstract rules at design-time. At run-time, a Drools compiler runs through the rule template and evaluates expressions to generate concrete rules. Given an event type class (e.g., &lt;i&gt;OrderEvent&lt;/i&gt;) and a &lt;i&gt;Rule&lt;/i&gt; object (e.g.,&amp;nbsp;&lt;i&gt;highValueOrderWidgetsIncRule&lt;/i&gt;), we can conceive the following rule template:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/50736b6ff70f75ae8bd7.js?file=template.drl&#34;&gt;&lt;/script&gt;A couple of things to observe:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Line 1: Declares that the DRL file is a rule template.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Line 3-4: &lt;i&gt;rule&lt;/i&gt; and &lt;i&gt;eventType&lt;/i&gt; are template parameters.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Line 8: &lt;i&gt;alertDecision&lt;/i&gt; is global variable which we write the outcome to should the rule evaluate to true.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Line 12: &lt;i&gt;@{row.rowNumber}&lt;/i&gt; is an in-built expression that makes the rule ID unique. This is useful for situations when you don&amp;rsquo;t know how many rules you&amp;rsquo;re going to have ahead of time. Note that this doesn&amp;rsquo;t apply to our example.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Line 14: &lt;i&gt;@{eventType}&lt;/i&gt; and &lt;i&gt;@{rule}&lt;/i&gt; MVEL expressions that are substituted with the template parameters at run-time.&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Line 16: Sets the property &lt;i&gt;doAlert&lt;/i&gt; to true to signal the application that the notification rule was fired.&lt;/li&gt;&lt;/ul&gt;Generating a rule from the template is a matter of instantiating&amp;nbsp;&lt;i&gt;ObjectDataCompiler&lt;/i&gt;&amp;nbsp;and passing as parameters:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;A map consisting of a&amp;nbsp;&lt;i&gt;Rule&lt;/i&gt; object (e.g.,&amp;nbsp;&lt;i&gt;highValueOrderWidgetsIncRule&lt;/i&gt;) and the name of the event class the Rule object pertains to&amp;nbsp;(e.g., &lt;i&gt;org.ossandme.event.OrderEvent&lt;/i&gt;)&lt;/li&gt;&lt;br /&gt;&lt;li&gt;The &lt;i&gt;template.drl&lt;/i&gt;&amp;nbsp;file&lt;/li&gt;&lt;/ol&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/50736b6ff70f75ae8bd7.js?file=Program(3).java&#34;&gt;&lt;/script&gt;Drools cannot evaluate a&amp;nbsp;&lt;i&gt;Rule&lt;/i&gt;&amp;nbsp;object&lt;i&gt;&amp;nbsp;&lt;/i&gt;in its current POJO form. In order to evaluate it, we override the &lt;i&gt;Rule&lt;/i&gt; class&amp;rsquo;s &lt;i&gt;toString()&lt;/i&gt; method to transform the POJO into a formal statement:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/50736b6ff70f75ae8bd7.js?file=Rule.java&#34;&gt;&lt;/script&gt;Before running the data through the template, Drools calls &lt;i&gt;toString()&lt;/i&gt;&amp;nbsp;on the template parameters. Calling &lt;i&gt;toString()&lt;/i&gt; on&amp;nbsp;&lt;i&gt;highValueOrderWidgetsIncRule&lt;/i&gt;&amp;nbsp;returns the statement:&amp;nbsp;&lt;i&gt;price &amp;gt; 5000.0 &amp;amp;&amp;amp; customer == &amp;lsquo;Widgets Inc.&amp;rsquo;&lt;/i&gt;. Going even further, if&amp;nbsp;we apply the template to the statement&amp;nbsp;and event type &lt;i&gt;OrderEvent&lt;/i&gt;, we would get the following generated rule:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/50736b6ff70f75ae8bd7.js?file=instance.drl&#34;&gt;&lt;/script&gt;The last step is to evaluate the rule:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/50736b6ff70f75ae8bd7.js?file=Program(4).java&#34;&gt;&lt;/script&gt;Finally, let&amp;rsquo;s put this all together. Don&amp;rsquo;t worry, &lt;a href=&#34;https://github.com/claudemamo/dynamic-drools-rules&#34; target=&#34;_blank&#34;&gt;a copy of the complete application&lt;/a&gt; can be found on GtiHub:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/50736b6ff70f75ae8bd7.js?file=Program.java&#34;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;span class=&#34;num&#34; style=&#34;font-family: Times, &#39;Times New Roman&#39;, serif;&#34;&gt;1: I&amp;rsquo;m ignoring the fact that most likely the rule is retrieved from a data store.&lt;/span&gt;&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Primer to AS2</title>
      <link>http://oncodesign.io/2015/05/15/a-primer-to-as2/</link>
      <pubDate>Fri, 15 May 2015 02:09:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2015/05/15/a-primer-to-as2/</guid>
      <description>&lt;p&gt;Check out my&lt;a href=&#34;http://www.modusbox.com/2015/04/a-primer-to-as2/&#34; target=&#34;_blank&#34;&gt; latest guest post&lt;/a&gt;&amp;nbsp;about AS2 on ModusBox&amp;rsquo;s blog.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Trials of Smooks</title>
      <link>http://oncodesign.io/2014/09/16/the-trials-of-smooks/</link>
      <pubDate>Tue, 16 Sep 2014 17:28:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2014/09/16/the-trials-of-smooks/</guid>
      <description>&lt;p&gt;The fact that I&amp;rsquo;m a hard to please guy explains why I rarely show appreciation for a tool. I easily get frustrated when a tool fails to meet the challenges it&amp;rsquo;s meant to solve. &lt;a href=&#34;http://www.smooks.org/&#34; target=&#34;_blank&#34;&gt;Smooks&lt;/a&gt; is one of the few tools I appreciate. It&amp;rsquo;s an invaluable transformation framework in the integrator&amp;rsquo;s arsenal. On a project I was on, I threw at Smooks [1] all manner of challenges, and one after another, Smooks overcame them without giving up a key requirement: &lt;b&gt;maintaining a low memory overhead during transformation&lt;/b&gt;. A shoutout to &lt;a href=&#34;http://www.smooks.org/mediawiki/index.php?title=Smooks_Team&#34; target=&#34;_blank&#34;&gt;Tom Fennelly and his team&lt;/a&gt; for bringing to us such a fantastic tool.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;&lt;b&gt;Trial I&lt;/b&gt;&lt;/h3&gt;&lt;br /&gt;The initial challenge I brought to Smooks was about taking a tilde delimited CSV file and map its records to POJOs:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=products.csv&#34;&gt;&lt;/script&gt;You can see the file has an unorthodox header in addition to a footer. Using Smooks&amp;rsquo;s built-in CSV &lt;b&gt;reader&lt;/b&gt;, I wrote concisely the Smooks config doing the mapping to POJOs:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=csv-to-pojos.xml&#34;&gt;&lt;/script&gt;What&amp;rsquo;s happening under the covers, and in general, is that the reader pulls data from a &lt;b&gt;source&lt;/b&gt; (e.g., &lt;i&gt;java.io.InputStream&lt;/i&gt;) to go on to produce a stream of SAX events. The reader I&amp;rsquo;m using above is expecting the source data to be structured as CSV and to consist of 4 columns. Let&amp;rsquo;s make things more concrete. Reading from the &lt;i&gt;products.csv&lt;/i&gt; file, the reader produces the following XML stream [2]:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=products-xml-stream.xml&#34;&gt;&lt;/script&gt;Listening to the stream of SAX events is the &lt;b&gt;visitor&lt;/b&gt;. A visitor listens to specific events from the stream to fire some kind of behaviour, typically transformation. With the&amp;nbsp;&lt;i&gt;singleBinding&lt;/i&gt;&amp;nbsp;element in the&amp;nbsp;&lt;i&gt;csv-to-pojos.xml&lt;/i&gt; config, the CSV reader pre-configures a JavaBean&amp;nbsp;visitor to listen for&amp;nbsp;&lt;i&gt;csv-record&lt;/i&gt; elements. On intercepting this element, the JavaBean visitor instantiates a&amp;nbsp;&lt;i&gt;org.ossandme.Product&lt;/i&gt;&amp;nbsp;object and binds its properties to &lt;i&gt;csv-record&lt;/i&gt;&amp;rsquo;s children element content. You&amp;rsquo;ll notice that I left&amp;nbsp;&lt;i&gt;Product&amp;rsquo;s&amp;nbsp;&lt;/i&gt;target properties unspecified in the config. The CSV reader assumes&amp;nbsp;&lt;i&gt;Product&lt;/i&gt;&amp;nbsp;follows JavaBean conventions and its properties are named the same as the defined CSV columns. Records disobeying the column definition are ignored. Consequently, I do not need to worry about the file&amp;rsquo;s header and footer.&lt;br /&gt;&lt;br /&gt;With the transformation configuration out of the way, I turned my attention to running the transformation on the CSV file from my Java code and process the &lt;i&gt;Product&lt;/i&gt; objects as they are instantiated and bound by Smooks:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=CsvToPojosTransformer.java&#34;&gt;&lt;/script&gt; &lt;br /&gt;&lt;h3&gt;&lt;b&gt;Trial II&lt;/b&gt;&lt;/h3&gt;&lt;br /&gt;A more complex transformation task I gave to Smooks was to load file records, holding a variable number of columns, into a database. As in the previous task, this file had a header as well as a footer:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=transactions.csv&#34;&gt;&lt;/script&gt;You&amp;rsquo;ll observe in the sample CSV file that records could be one of three types as denoted by the first column: &lt;i&gt;TH&lt;/i&gt;, &lt;i&gt;TB&lt;/i&gt; or &lt;i&gt;TF&lt;/i&gt;. The CSV reader, as it transforms and pushes records to the XML stream, can be customised such that it renames the&amp;nbsp;&lt;i&gt;csv-record&lt;/i&gt;&amp;nbsp;holder to the record&amp;rsquo;s primary column:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=transactions-to-db(1).xml&#34;&gt;&lt;/script&gt;As we&amp;rsquo;ll see later, the above config permits Smooks to distinguish between the different record types. Given the sample file &lt;i&gt;transactions.csv&lt;/i&gt;, the reader I&amp;rsquo;ve configured produces the following stream:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=transactions-xml-stream.xml&#34;&gt;&lt;/script&gt;&lt;i&gt;UNMATCHED&lt;/i&gt;&amp;nbsp;elements represent the file&amp;rsquo;s header and footer. A CSV record having &lt;i&gt;TH&lt;/i&gt; in the first field will trigger the reader to create a &lt;i&gt;TH&lt;/i&gt; element holding the other record fields. The same logic goes for&amp;nbsp;&lt;i&gt;TB&lt;/i&gt; and &lt;i&gt;TF&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;Database visitors load the records. However, since these visitors are limited to binding data from POJOs, I first must turn the XML mapped records from the stream into said POJOs. The CSV reader doesn&amp;rsquo;t know how to bind variable field records to POJOs so I configure the mapping myself:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=transactions-to-db(2).xml&#34;&gt;&lt;/script&gt;Given what we&amp;rsquo;ve learnt about Smooks, we can deduce what&amp;rsquo;s happening here. The JavaBean visitor for lines 10 till 17 has a &lt;b&gt;selector&lt;/b&gt; (i.e, &lt;i&gt;createOnElement&lt;/i&gt;)&amp;nbsp;for the element&amp;nbsp;&lt;i&gt;TH&lt;/i&gt;. A selector is&amp;nbsp;a quasi XPath expression applied on XML elements as they come through the stream. On viewing&amp;nbsp;&lt;i&gt;TH&lt;/i&gt;, the visitor will:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Instantiate a &lt;i&gt;HashMap&lt;/i&gt;.&lt;/li&gt;&lt;br /&gt;&lt;li&gt;Iterate through the &lt;i&gt;TH&lt;/i&gt; fragment. If an element inside the fragment matches the selector set in a &lt;i&gt;data&lt;/i&gt; attribute, then (a) a map entry is created, (b) bound to the element content, and &amp;copy; put in the map.&lt;/li&gt;&lt;br /&gt;&lt;li&gt;Add the map to the Smooks bean context which is identified by the name set in &lt;i&gt;beanID&lt;/i&gt;. The map overwrites any previous map in the context with the same ID. This makes sense since we want to prevent objects from accumulating in memory.&lt;/li&gt;&lt;/ol&gt;The database visitors reference the maps in the bean context:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=transactions-to-db(3).xml&#34;&gt;&lt;/script&gt;The insert statements are bound to the map entry values and are executed &lt;b&gt;after&lt;/b&gt;&amp;nbsp;the element, the &lt;i&gt;executeOnElement&lt;/i&gt; selector points to, is processed. The next step is to configure a datasource for the database visitors (lines 47-49):&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=transactions-to-db(4).xml&#34;&gt;&lt;/script&gt;Last but not least, the Java code to kick off the data load:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=CsvToDbTransformer.java&#34;&gt;&lt;/script&gt; &lt;br /&gt;&lt;h3&gt; Trial III&lt;/h3&gt;&lt;br /&gt;The next challenge for Smooks makes the previous ones look like child&amp;rsquo;s play. The goal: transform an XML stream to a CSV file that is eventually uploaded to an FTP server. The input:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=accounts-stream.xml&#34;&gt;&lt;/script&gt;The desired output:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=accounts.csv&#34;&gt;&lt;/script&gt;Considering the CSV could be large in size, my requirement was for Smooks to write the transformed content to a &lt;a href=&#34;http://docs.oracle.com/javase/8/docs/api/java/io/PipedOutputStream.html&#34; target=&#34;_blank&#34;&gt;&lt;i&gt;PipedOutputStream&lt;/i&gt;&lt;/a&gt;. An FTP library would read from the &lt;i&gt;PipedOutputStream&lt;/i&gt;&amp;rsquo;s connected &lt;a href=&#34;http://docs.oracle.com/javase/8/docs/api/java/io/PipedInputStream.html&#34; target=&#34;_blank&#34;&gt;&lt;i&gt;PipedInputStream&lt;/i&gt;&lt;/a&gt;, and write the streamed content to a file. To this end, I wrote the class running the transformation as follows:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=XmlToCsvTransformer(1).java&#34;&gt;&lt;/script&gt;My focus then turned to the XML-to-CSV mapping configuration. After deliberation, I reluctantly settled to use the&amp;nbsp;&lt;a href=&#34;http://freemarker.org/&#34; target=&#34;_blank&#34;&gt;FreeMarker&lt;/a&gt; visitor for writing the CSV. I considered as an alternative to develop a visitor specialised for this type of transformation but time constraints made this unfeasible. The FreeMarker visitor, like the database one, cannot read directly off the XML stream. Instead, it can read from DOM and POJOs. So I decide to use the DOM visitor such that it creates DOMs from &lt;i&gt;record&lt;/i&gt;&amp;nbsp;elements found within the input stream:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=xml-to-csv(1).xml&#34;&gt;&lt;/script&gt;I then configured the FreeMarker visitor to apply the CSV template on seeing the element&amp;nbsp;&lt;i&gt;record&lt;/i&gt;&amp;nbsp;in the stream:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=xml-to-csv(2).xml&#34;&gt;&lt;/script&gt;Below is a simplified version of what I had in real life in&lt;i&gt;&amp;nbsp;account.ftl&lt;/i&gt;&amp;nbsp;(note the last line of the template must be a newline):&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=account.ftl&#34;&gt;&lt;/script&gt;An additional complexity I had to consider were the CSV&amp;rsquo;s header and footer. Apart from being structured differently than the rest of the records, the header had to contain the current date whereas, for the footer, the total record count. What I did for the header was to bind the current date from my Java code to Smooks&amp;rsquo;s bean context (lines 27-30 and 38):&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=XmlToCsvTransformer(2).java&#34;&gt;&lt;/script&gt;The date is then referenced from the Smooks config (lines 9-12):&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=xml-to-csv(3).xml&#34;&gt;&lt;/script&gt;With respect to the above config, at the start of the XML stream, FreeMarker writes the header to the output stream (i.e., &lt;i&gt;PipedOutputStream&lt;/i&gt;):&lt;br /&gt;&lt;br /&gt;&lt;i&gt;000000Card Extract &amp;nbsp; &lt;/i&gt;[current date]&lt;br /&gt;&lt;br /&gt;&lt;i&gt;&amp;lt;?TEMPLATE-SPLIT-PI?&amp;gt;&lt;/i&gt;&amp;nbsp;is an embedded Smooks instruction that applies&amp;nbsp;&lt;i&gt;account.ftl&lt;/i&gt;&amp;nbsp;to &lt;i&gt;record&lt;/i&gt; elements after the header.&lt;br /&gt;&lt;br /&gt;Adding the record count to the footer is just a matter of configuring the Calculator visitor to maintain a counter in the bean context and referencing that counter from the template:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=xml-to-csv(4).xml&#34;&gt;&lt;/script&gt; &lt;br /&gt;&lt;h3&gt; Trial IV&lt;/h3&gt;&lt;br /&gt;The final challenge Smooks had to go against was to read from a&amp;nbsp;&lt;i&gt;java.util.Iterator&lt;/i&gt;&amp;nbsp;of maps and, like the previous task, write the transformed output to a stream in CSV format. Unlike the &lt;i&gt;InputStream&lt;/i&gt;&amp;nbsp;that Smooks read from the other tasks, Smooks doesn&amp;rsquo;t have a reader that is capable of writing a properly structured XML doc from an iterator of maps. So I&amp;rsquo;m left with writing my own reader:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=MapIteratorSourceReader.java&#34;&gt;&lt;/script&gt;The custom reader is hooked into Smooks as follows (line 5):&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=map-iterator-to-csv.xml&#34;&gt;&lt;/script&gt;Finally, passing the iterator to Smooks for transformation consists of setting a &lt;i&gt;JavaSource&lt;/i&gt; parameter, holding the iterator, on &lt;i&gt;filterSource(&amp;hellip;)&lt;/i&gt; &amp;nbsp;(line 27):&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/e80c8add45b776e0a0a5.js?file=MapIteratorToCsvTransformer.java&#34;&gt;&lt;/script&gt; &lt;br /&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;span class=&#34;num&#34; style=&#34;font-family: &amp;quot;times&amp;quot; , &amp;quot;times new roman&amp;quot; , serif;&#34;&gt;1: The Smooks version I used was 1.5.2.&lt;/span&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;span class=&#34;num&#34; style=&#34;font-family: &amp;quot;times&amp;quot; , &amp;quot;times new roman&amp;quot; , serif;&#34;&gt;2: You might be wondering how I know for certain the XML document shown is the one actually produced by Smooks. I know because of Smooks&amp;rsquo;s &lt;a href=&#34;http://www.smooks.org/mediawiki/index.php?title=V1.5:Smooks_v1.5_User_Guide#Checking_the_Smooks_Execution_Process&#34;&gt;HtmlReportGenerator&lt;/a&gt; class.&lt;/span&gt;&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Safely Prevent Template Caching in AngularJS</title>
      <link>http://oncodesign.io/2014/02/19/safely-prevent-template-caching-in-angularjs/</link>
      <pubDate>Wed, 19 Feb 2014 15:21:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2014/02/19/safely-prevent-template-caching-in-angularjs/</guid>
      <description>&lt;p&gt;AngularJS&amp;rsquo;s &lt;i&gt;&lt;a href=&#34;http://docs.angularjs.org/api/ng/service/$templateCache&#34; target=&#34;_blank&#34;&gt;$templateCache&lt;/a&gt;&lt;/i&gt; can be a pain in the ass. Sometimes we don&amp;rsquo;t want templates to be cached. A quick Internet search to disable caching gives the following workaround:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/9092047.js?file=app(1).js&#34;&gt;&lt;/script&gt; But as I have learnt with the &lt;a href=&#34;http://angular-ui.github.io/bootstrap/&#34; target=&#34;_blank&#34;&gt;UI Bootsrap module&lt;/a&gt;, this may cause AngularJS modules that use &lt;i&gt;$templateCache&lt;/i&gt; to break. A solution is to tweak the above workaround so that new cache entries are removed on route change instead of indiscriminately removing all entries:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/9092047.js?file=app(2).js&#34;&gt;&lt;/script&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamically Create BitCoin Wallets &amp; Payment Pages on Coinbase in Ruby</title>
      <link>http://oncodesign.io/2014/02/11/dynamically-create-bitcoin-wallets--payment-pages-on-coinbase-in-ruby/</link>
      <pubDate>Tue, 11 Feb 2014 16:53:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2014/02/11/dynamically-create-bitcoin-wallets--payment-pages-on-coinbase-in-ruby/</guid>
      <description>&lt;p&gt;Last weekend, as part of my new year&amp;rsquo;s resolution to dedicate some time to good causes, I participated in &lt;a href=&#34;http://hack4good.io/&#34; target=&#34;_blank&#34;&gt;Hack4good&lt;/a&gt;: a global 48 hour hackathon aimed at bringing ideas for the social good into life. In Malta, our team brought forward a crowd funding solution for charitable fundraisers with &lt;b&gt;minimal&lt;/b&gt; transaction fees. To this end, we selected &lt;a href=&#34;https://www.youtube.com/watch?v=Um63OQz3bjo&#34; target=&#34;_blank&#34;&gt;BitCoin&lt;/a&gt; as the donation currency and &lt;a href=&#34;https://coinbase.com/&#34; target=&#34;_blank&#34;&gt;Coinbase&lt;/a&gt; to host fundraise donations.&lt;br /&gt;&lt;br /&gt;One requirement in our project was to have Coinbase automatically issue a BitCoin wallet to each&amp;nbsp;fundraiser. To further complicate matters, we wanted to generate a Coinbase payment page that allows the donor to&amp;nbsp;transfer his BitCoins to the fundraiser&amp;rsquo;s wallet:&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-9URc-UR2bBg/UvouzVJqjUI/AAAAAAAAAHY/VTX_OUmsRW4/s1600/coinbase-payment-page.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-9URc-UR2bBg/UvouzVJqjUI/AAAAAAAAAHY/VTX_OUmsRW4/s1600/coinbase-payment-page.png&#34; height=&#34;400&#34; width=&#34;392&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;a href=&#34;https://coinbase.com/api/doc&#34; target=&#34;_blank&#34;&gt;Coinbase&amp;rsquo;s awesome API&lt;/a&gt; permitted us to do both things with very little effort. Since we developed the solution in Ruby on Rails 4, I&amp;rsquo;ll show you the code of how we accomplished this using a &lt;a href=&#34;https://github.com/claudemamo/coinbase-ruby&#34; target=&#34;_blank&#34;&gt;forked version of Coinbase API&amp;rsquo;s Ruby client&lt;/a&gt; [1]:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/8918100.js?file=fundraiser_controller(1).rb&#34;&gt;&lt;/script&gt;The&amp;nbsp;&lt;i&gt;create()&lt;/i&gt;&amp;nbsp;controller action does numerous things so let&amp;rsquo;s dissect it piece by piece. The action instantiates the Coinbase client with our API key: this key is created in Coinbase&amp;rsquo;s account settings page. The client object&amp;rsquo;s &lt;i&gt;create_user(&amp;hellip;)&lt;/i&gt; method is then invoked to make a wallet in addition to a Coinbase account for the fundraiser. The email address and password parameters are used by the end-user to access his fundraiser wallet on Coinbase. &lt;i&gt;COINBASE_CLIENT_SECRET&lt;/i&gt;, linked to our API key, is passed as a parameter so that we can automatically grant ourselves merchant permissions on the created user account. These permissions are needed to dynamically generate the payment page on behalf of the user.&lt;br /&gt;&lt;br /&gt;Making the call to Coinbase to generate the payment page requires that we follow the OAuth 2 protocol [2]. Fortunately, an&amp;nbsp;&lt;a href=&#34;https://rubygems.org/gems/oauth2&#34; target=&#34;_blank&#34;&gt;OAuth 2 Ruby library&lt;/a&gt; exists. So we go ahead and use the library to instantiate an OAuth client, passing &lt;i&gt;COINBASE_API_KEY&lt;/i&gt; and &lt;i&gt;COINBASE_API_SECRET&lt;/i&gt; as parameters. Before we ask Coinbase to create a payment page on the user&amp;rsquo;s behalf, an&amp;nbsp;&lt;i&gt;AccessToken&lt;/i&gt; object is constructed with the access token obtained from&amp;nbsp;&lt;i&gt;coinbase&lt;/i&gt;.&lt;i&gt;create_user(&amp;hellip;)&lt;/i&gt;&amp;nbsp;and the OAuth client we have just instantiated. After this, we use the newly constructed&amp;nbsp;&lt;i&gt;oauth_token&lt;/i&gt;&amp;nbsp;object to post a request to&amp;nbsp;&lt;i&gt;&lt;a href=&#34;https://coinbase.com/api/v1/buttons&#34;&gt;https://coinbase.com/api/v1/buttons&lt;/a&gt;.&lt;/i&gt;&amp;nbsp;Note that &lt;i&gt;JSON_CREATE_PAYMENT_PAGE&lt;/i&gt;&amp;rsquo;s value is sent as the HTTP body.&lt;br /&gt;&lt;br /&gt;All I need from the JSON response returned from the API call is the payment page code. This code lets Coinbase know which payment page to display. We persist this code along with the fundraiser details so that we can retrieve them later when we show the fundraiser to a potential donor:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/8918100.js?file=fundraiser_controller(2).rb&#34;&gt;&lt;/script&gt;Here is view associated with the above action:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/8918100.js?file=show.html.erb&#34;&gt;&lt;/script&gt;The view gets the page code from&amp;nbsp;&lt;i&gt;@fundraiser.coinbase_page_code&lt;/i&gt; and sets the necessary HTML attributes with this value. &lt;i&gt;button.js&lt;/i&gt;&amp;nbsp;is a script provided by Coinbase that styles the anchor element and opens the fundraising donation page tied to the page code when the anchor is clicked:&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-216Y0Y3NHe8/Uvoo9rAJu5I/AAAAAAAAAHM/yQWlxI-KSEE/s1600/donation_large.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-216Y0Y3NHe8/Uvoo9rAJu5I/AAAAAAAAAHM/yQWlxI-KSEE/s1600/donation_large.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;The final step is to add the OAuth 2 and Coinbase dependencies to the project Gemfile:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/8918100.js?file=Gemfile&#34;&gt;&lt;/script&gt;&lt;br /&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;span class=&#34;num&#34; style=&#34;font-family: Times, &#39;Times New Roman&#39;, serif;&#34;&gt;1: We forked &lt;a href=&#34;https://github.com/coinbase/coinbase-ruby&#34; target=&#34;_blank&#34;&gt;Coinbase&amp;rsquo;s Ruby client&lt;/a&gt; because &lt;i&gt;create_user(&amp;hellip;)&lt;/i&gt;&amp;nbsp;didn&amp;rsquo;t support&amp;nbsp;&lt;a href=&#34;https://coinbase.com/api/doc/1.0/users/create.html&#34; target=&#34;_blank&#34;&gt;client ID&lt;/a&gt;.&lt;/span&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;span class=&#34;num&#34; style=&#34;font-family: Times, &#39;Times New Roman&#39;, serif;&#34;&gt;2: You need to register your application on Coinbase before you can gain rights to manage user accounts through OAuth.&amp;nbsp;&lt;/span&gt;&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scaling up Mule with Async Request Handling/Continuations</title>
      <link>http://oncodesign.io/2014/01/07/scaling-up-mule-with-async-request-handling/continuations/</link>
      <pubDate>Tue, 07 Jan 2014 14:01:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2014/01/07/scaling-up-mule-with-async-request-handling/continuations/</guid>
      <description>&lt;p&gt;Non-blocking I/O servers such as Node.js are appealing because, when compared to blocking I/O servers, they utilise less threads to perform the same tasks under the same load. Less threads mean more efficient use of resources (e.g., smaller memory footprint) and better performance (e.g., reduced no. of context switches between threads). Let&amp;rsquo;s take a stab at having non-blocking I/O behaviour in Mule. Consider the following Mule 3.4 application calling an HTTP service:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/8289429.js?file=mule-config(1).xml&#34;&gt;&lt;/script&gt;Wrapping the&amp;nbsp;&lt;i&gt;async&lt;/i&gt;&amp;nbsp;processor around&amp;nbsp;&lt;i&gt;http:outbound-endpoint&lt;/i&gt;&amp;nbsp;prevents the receiver thread from blocking on the outgoing HTTP call. But this &lt;b&gt;kind&lt;/b&gt; of asynchronous behaviour causes the service&amp;rsquo;s reply to be ignored: certainly not what we want for the common case. Moreover, the&amp;nbsp;&lt;i&gt;async&lt;/i&gt;&amp;nbsp;processor borrows a thread from some thread pool to carry out the blocking HTTP call, preventing the borrowed thread from doing any useful work while being blocked.&lt;br /&gt;&lt;br /&gt;The aforementioned problems can generally be solved by replacing the blocking I/O library with a non-blocking version and&amp;nbsp;&lt;b&gt;&lt;a href=&#34;http://wiki.eclipse.org/Jetty/Feature/Continuations&#34; target=&#34;_blank&#34;&gt;Asynchronous Request Handling&lt;/a&gt;&lt;/b&gt;&amp;nbsp;(a.k.a continuations). Async request handling is a threading model where a thread serving a client request can be suspended and returned to its respective thread pool; free to serve other client requests. Typically the thread would be suspended after sending out a request to a remote service or kicking off a long-running computation. Although the suspended thread has forgotten about the client, the server has not. It knows the client is still waiting for a reply. For this reason, a thread can pick up where the suspended tread has left off and deliver the reply back to the client. Normally this would happen in the context of a callback.&lt;br /&gt;&lt;br /&gt;Awesome! Let&amp;rsquo;s implement this in every place where blocking I/O is present. Not so fast. First, a library supporting a non-blocking alternative to what you already have in your solution must be available. Second, to my knowledge, the only Mule transport that provides async request handling&amp;nbsp;is &lt;a href=&#34;http://www.mulesoft.org/documentation/display/current/Jetty+Transport+Reference&#34; target=&#34;_blank&#34;&gt;Jetty&lt;/a&gt;. So for this to work, the Jetty inbound-endpoint&amp;nbsp;processor must be used as the message source:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/8289429.js?file=mule-config(2).xml&#34;&gt;&lt;/script&gt;Furthermore, as shown above,&amp;nbsp;async&amp;nbsp;request handling&amp;nbsp;must be turned on by setting &lt;i&gt;useContinuations&lt;/i&gt; to true on the&amp;nbsp;Jetty connector.&lt;br /&gt;&lt;br /&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;Calling an HTTP service is a fine example where we can put async&amp;nbsp;request handling to good use. The initial step is to find an HTTP client library implementing a non-blocking API [1]. I&amp;rsquo;ll opt for &lt;a href=&#34;http://hc.apache.org/httpcomponents-asyncclient-4.0.x/&#34; target=&#34;_blank&#34;&gt;Apache&amp;nbsp;HttpAsyncClient&lt;/a&gt;.&lt;/div&gt;&lt;br /&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;The next step is to develop a message processor that (1) uses HttpAsyncClient to call a service, (2) registers a callback to resume processing of the client request on receiving the HTTP service reply, and (3) immediately returns the thread to its thread pool upon sending asynchronously the HTTP request. Such a processor will require special abilities so I&amp;rsquo;ll extend my processor from&amp;nbsp;&lt;i&gt;&lt;a href=&#34;http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/processor/AbstractInterceptingMessageProcessor.html&#34; target=&#34;_blank&#34;&gt;AbstractInterceptingMessageProcessor&lt;/a&gt;&lt;/i&gt;:&lt;/div&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/8289429.js?file=AhcProcessor(1).java&#34;&gt;&lt;/script&gt;By inheriting from &lt;i&gt;AbstractInterceptingMessageProcessor&lt;/i&gt;, I can invoke the next processor in the flow from my callback. Speaking of callbacks, here is the snippet concerning the HTTP client:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/8289429.js?file=AhcProcessor(2).java&#34;&gt;&lt;/script&gt;Lines 10-13 initialise the HTTP client and set the server address to wherever we&amp;rsquo;re going to send the request to. Line 15 sends asynchronously the request, and registers the callback that will handle the reply. Other than the usual stuff of reading from the response stream (lines 19-22), observe that on line 23 the subsequent flow processor in invoked on a&amp;nbsp;&lt;b&gt;different&lt;/b&gt;&amp;nbsp;thread. Line 24 tells Jetty that the flow&amp;rsquo;s output message is to be used as the reply for the end-user.&lt;br /&gt;&lt;br /&gt;One additional item in the list is left: freeing the thread after invoking asynchronously the HTTP client&amp;rsquo;s &lt;i&gt;execute(&amp;hellip;)&lt;/i&gt; method. Returning &lt;i&gt;null&lt;/i&gt; from the &lt;i&gt;process(&amp;hellip;)&lt;/i&gt; method will do the job (line 40):&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/8289429.js?file=AhcProcessor(3).java&#34;&gt;&lt;/script&gt;Finally, we can hook up everything together:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/8289429.js?file=mule-config(3).xml&#34;&gt;&lt;/script&gt;The &lt;a href=&#34;https://github.com/claudemamo/asynchronous-request-handling&#34; target=&#34;_blank&#34;&gt;complete example&lt;/a&gt; is found on GitHub.&lt;br /&gt;&lt;br /&gt;Hopefully async request handling will someday be &lt;a href=&#34;https://www.mulesoft.org/jira/browse/MULE-7214&#34; target=&#34;_blank&#34;&gt;part of Mule&amp;rsquo;s&amp;nbsp;default behaviour&lt;/a&gt;. Imagine how useful it would be to call almost any service (e.g., HTTP, JMS, VM) synchronously knowing fully well that behind the scenes Mule is taking care of making every remote call non-blocking.&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: Times, Times New Roman, serif; font-size: small;&#34;&gt;&lt;span class=&#34;num&#34;&gt;1:&amp;nbsp;&lt;/span&gt;&lt;span style=&#34;text-align: justify;&#34;&gt;A client library implementation should be based on the &lt;a href=&#34;http://www.dre.vanderbilt.edu/~schmidt/PDF/reactor-siemens.pdf&#34; target=&#34;_blank&#34;&gt;Reactor pattern&lt;/a&gt; otherwise we would be going back to the original problem of many blocking threads.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Log4j 2 Memory-Mapped File Appender</title>
      <link>http://oncodesign.io/2013/12/23/log4j-2-memory-mapped-file-appender/</link>
      <pubDate>Mon, 23 Dec 2013 11:38:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2013/12/23/log4j-2-memory-mapped-file-appender/</guid>
      <description>&lt;p&gt;During the weekend I dug into Java NIO, specifically, mapping files to memory to reduce I/O time. What&amp;rsquo;s more, since I had a lot of free time on my hands, I developed a&amp;nbsp;&lt;a href=&#34;http://logging.apache.org/log4j/2.x/&#34; target=&#34;_blank&#34;&gt;Log4j 2&lt;/a&gt;&amp;nbsp;&lt;a href=&#34;https://issues.apache.org/jira/browse/LOG4J2-431&#34; target=&#34;_blank&#34;&gt;memory-mapped file appender&lt;/a&gt;. On my machine, performance tests running on a single thread using the MemoryMappedFile appender show an improvement by a factor of 3 when compared to the&amp;nbsp;&lt;a href=&#34;http://logging.apache.org/log4j/2.x/manual/appenders.html#RandomAccessFileAppender&#34; target=&#34;_blank&#34;&gt;RandomAccessFile&lt;/a&gt;&amp;nbsp;appender.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Apache Kafka for Event Sourcing</title>
      <link>http://oncodesign.io/2013/12/11/apache-kafka-for-event-sourcing/</link>
      <pubDate>Wed, 11 Dec 2013 21:01:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2013/12/11/apache-kafka-for-event-sourcing/</guid>
      <description>&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;a href=&#34;http://martinfowler.com/eaaDev/EventSourcing.html&#34; target=&#34;_blank&#34;&gt;Event Sourcing&lt;/a&gt; is a pattern intended for &#34;&lt;i&gt;capturing all changes to an application state as a sequence of events&lt;/i&gt;&#34;. As explained by Fowler, the pattern is useful when you want the ability to completely rebuild the application state, perform temporal querying, or replay events. The &lt;a href=&#34;http://www.infoq.com/presentations/LMAX&#34; target=&#34;_blank&#34;&gt;LMAX platform&lt;/a&gt; is a famous example where Event Sourcing is applied to keep all application state in-memory and consequently contributing to the system&#39;s surprisingly high throughput and low latency.&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;While investigating the architectural components of &lt;a href=&#34;http://samza.incubator.apache.org/&#34; target=&#34;_blank&#34;&gt;Samza&lt;/a&gt;, I came across a component that can be of great help when implementing Event Sourcing:&amp;nbsp;&lt;a href=&#34;https://kafka.apache.org/&#34; target=&#34;_blank&#34;&gt;Apache Kafka&lt;/a&gt;. &lt;a href=&#34;http://research.microsoft.com/en-us/um/people/srikanth/netdb11/netdb11papers/netdb11-final12.pdf&#34; target=&#34;_blank&#34;&gt;Created&lt;/a&gt; by the folks at LinkedIn as a solution to their log processing requirements, Kafka is a broker with message replay built-in.&lt;/div&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&#34;text-align: left;&#34;&gt;Kafka consumers receive messages from publish/subscribe channels known as topics. A topic is divided into user-defined partitions where a partition can serve messages only to a single consumer process. Balancing the message load between consumers is a matter of adding more partitions to the topic, assigning those partitions to other consumer instances, and finally, publishing messages to all topic partitions in a round robin fashion.&lt;/div&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-kvjAcN0vFfg/Uqb4m63H3_I/AAAAAAAAAGw/zJ7XrGdjiDA/s1600/kafka-concepts.png&#34; imageanchor=&#34;1&#34; style=&#34;clear: left; float: left; margin-bottom: 1em; margin-right: 1em; text-align: left;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-kvjAcN0vFfg/Uqb4m63H3_I/AAAAAAAAAGw/zJ7XrGdjiDA/s640/kafka-concepts.png&#34; height=&#34;356&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;What fascinates about Kafka is that at any point in time a consumer can rewind back through the history of messages and re-consume messages at a particular offset. In the above diagram, Consumer B can consume the latest messages or replay messages, say, starting from offset 1.&amp;nbsp;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;At face value we could be forgiven to think that a broker with in-built message replay would have trouble achieving high throughput for large message volumes. After all, Kafka is retaining&amp;nbsp;unconsumed&amp;nbsp;as well as consumed messages on disk: presumably costlier than simply keeping unconsumed messages in memory. However, a &lt;a href=&#34;http://kafka.apache.org/documentation.html#design&#34; target=&#34;_blank&#34;&gt;few&lt;/a&gt; clever design decisions, such as relying on the OS page cache and minimising random disk I/O, gave LinkedIn engineers impressive throughput results when comparing Kafka against both ActiveMQ and RabbitMQ.&lt;/div&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;With the basic concepts and performance considerations out of the way, let me illustrate my point about Kafka&#39;s suitability for Event Sourcing by giving a &lt;a href=&#34;https://github.com/claudemamo/kafka-replays&#34; target=&#34;_blank&#34;&gt;code example&lt;/a&gt;:&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/7840143.js?file=MyProducer.java&#34;&gt;&lt;/script&gt;The above producer publishes, for a number of times, a message to the topic &lt;i&gt;ossandme &lt;/i&gt;on partition 0. In particular, it creates a message by instantiating the&amp;nbsp;&lt;i&gt;KeyedMessage&lt;/i&gt; class with the following parameters (line 19):&lt;/div&gt;&lt;/div&gt;&lt;ul&gt;&lt;li style=&#34;text-align: left;&#34;&gt;Name of the topic to which the message is published.&lt;/li&gt;&lt;li style=&#34;text-align: left;&#34;&gt;ID of the partition the message will sit on.&lt;/li&gt;&lt;li style=&#34;text-align: left;&#34;&gt;Message content, in this case, the time the message was published.&lt;/li&gt;&lt;/ul&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;The following consumer pulls messages from the&amp;nbsp;topic&amp;nbsp;&lt;i&gt;ossandme&lt;/i&gt;:&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/7840143.js?file=MyConsumer.java&#34;&gt;&lt;/script&gt;For each message received, the application outputs the message&#39;s offset on partition 0 in addition to its content (line 50). The first thing to observe is that I&#39;ve programmed against Kafka&#39;s low-level&amp;nbsp;&lt;a href=&#34;http://kafka.apache.org/documentation.html#simpleconsumerapi&#34; target=&#34;_blank&#34;&gt;SimpleConsumer&lt;/a&gt;&amp;nbsp;API. Alternatively, I could have opted for the&amp;nbsp;&lt;a href=&#34;http://kafka.apache.org/documentation.html#highlevelconsumerapi&#34; target=&#34;_blank&#34;&gt;High Level Consumer&lt;/a&gt;&amp;nbsp;API to reduce development time. I chose the former because with the latter I was unable to find a way to replay any set of messages I wanted.&amp;nbsp;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;Event Sourcing comes into play when an exception occurs. On exception, the application rewinds back to the first message in the partition and re-attempts to process all of the partition&#39;s messages (line 24). I like the fact that, to get this type of behaviour, I didn&#39;t have to introduce a database but simply leveraged the broker&#39;s message replay capability. Without a database, I&#39;ve one less moving part to think about in my architecture.&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;div style=&#34;text-align: left;&#34;&gt;Kafka is a young project and I&#39;m interested to see how it matures. I&#39;m keen to hear people&#39;s experiences using Kafka and whether it proved to be the right solution for them. As the project matures, I suspect we&#39;ll hear more often about Kafka in our technical discussions along with the other, more established, open source brokers.&lt;/div&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bridging Mule and MSMQ with ZeroMQ</title>
      <link>http://oncodesign.io/2013/08/27/bridging-mule-and-msmq-with-zeromq/</link>
      <pubDate>Tue, 27 Aug 2013 17:36:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2013/08/27/bridging-mule-and-msmq-with-zeromq/</guid>
      <description>&lt;p&gt;Hearing the words Mule and Microsoft&amp;rsquo;s MSMQ in the same sentence sends a shiver down my spine. I remember once, Mule guru &lt;a href=&#34;http://blogs.mulesoft.org/author/john-demic/&#34; target=&#34;_blank&#34;&gt;John D&amp;rsquo;Emic&lt;/a&gt; and me had spent a considerable amount of time and patience getting Mule and MSMQ to talk to each other through DCOM. The major factor that contributed to this unpleasant experience was our ignorance of the numerous security measures imposed by Windows to restrict DCOM access. The morale of this story is unless you have a veteran Windows administrator at your disposal, avoid the DCOM route.&lt;br /&gt;&lt;br /&gt;So which choices do we have other than DCOM? JNI sounds promising but you are then sacrificing Mule&amp;rsquo;s platform independence. Here&amp;rsquo;s an idea: introduce a &lt;a href=&#34;http://www.eaipatterns.com/MessagingBridge.html&#34; target=&#34;_blank&#34;&gt;messaging bridge&lt;/a&gt; between Mule and MSMQ. The bridge can be implemented in any language that facilitates interaction with MSMQ. C# is an attractive option.&lt;br /&gt;&lt;br /&gt;We still have to consider which middleware to use for exchanging messages between the bridge and Mule. There are many alternatives and among them is &lt;a href=&#34;http://zeromq.org/&#34; target=&#34;_blank&#34;&gt;ZeroMQ&lt;/a&gt;. I think ZeroMQ is a good candidate for several reasons:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;It supports asynchronous communication&lt;/li&gt;&lt;li&gt;You&amp;rsquo;re not adding another component to your architecture&lt;/li&gt;&lt;li&gt;It&amp;rsquo;s well documented in addition to having a low learning curve&lt;/li&gt;&lt;li&gt;A ZeroMQ&amp;nbsp;&lt;a href=&#34;https://github.com/claudemamo/mule-transport-zeromq&#34; target=&#34;_blank&#34;&gt;transport&lt;/a&gt; [1] and &lt;a href=&#34;https://github.com/zeromq/clrzmq&#34; target=&#34;_blank&#34;&gt;binding&lt;/a&gt;&amp;nbsp;are available for Mule and C# respectively&lt;/li&gt;&lt;li&gt;It will more than likely satisfy your message throughput requirements&lt;/li&gt;&lt;/ul&gt;In as little as 15 minutes I developed a simple bridge in C#:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6345631.js?file=bridge.cs&#34;&gt;&lt;/script&gt; The above code should be self-explanatory but I&amp;rsquo;ve put comments for your convenience.&lt;br /&gt;&lt;br /&gt;Here&amp;rsquo;s the Mule 3 app dispatching messages to the bridge:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6345631.js?file=mule-config.xml&#34;&gt;&lt;/script&gt;On receiving an HTTP request, Mule leverages the ZeroMQ transport to send asynchronously the request&amp;rsquo;s payload to the bridge.&lt;br /&gt;&lt;br /&gt;In all likelihood, the illustrated bridge code for Mule-MSMQ interoperability won&amp;rsquo;t serve all your needs. I can think about a dozen features that a developer would want such as destination queues resolved at run-time, an agreed format for message content, and etc. But hey, at least it&amp;rsquo;s a start :-)&lt;br /&gt;&lt;br /&gt;&lt;div style=&#34;text-align: justify;&#34;&gt;&lt;span style=&#34;font-family: Times, Times New Roman, serif; font-size: small;&#34;&gt;&lt;span class=&#34;num&#34;&gt;1:&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;font-family: Times, Times New Roman, serif;&#34;&gt;I&amp;rsquo;ve recently replaced the transport&amp;rsquo;s ZeroMQ C++ library with a &lt;a href=&#34;https://github.com/zeromq/jeromq&#34; target=&#34;_blank&#34;&gt;pure Java implementation&lt;/a&gt; of ZeroMQ.&lt;/span&gt;&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JRuby Complex Classes in Java Method Signatures</title>
      <link>http://oncodesign.io/2013/08/19/jruby-complex-classes-in-java-method-signatures/</link>
      <pubDate>Mon, 19 Aug 2013 11:41:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2013/08/19/jruby-complex-classes-in-java-method-signatures/</guid>
      <description>&lt;p&gt;As documented in the JRuby wiki,&amp;nbsp;&lt;i&gt;&lt;a href=&#34;https://github.com/jruby/jruby/wiki/GeneratingJavaClasses#generating-java-classes-ahead-of-time&#34; target=&#34;_blank&#34;&gt;java_signature&lt;/a&gt;&lt;/i&gt;&amp;nbsp;changes a method&amp;rsquo;s signature to match the signature string passed to it:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6262775.js?file=example(1).rb&#34;&gt;&lt;/script&gt;Observe that the classes in the method signature&amp;nbsp;are primitive. What if we use a complex class as a parameter type?&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6262775.js?file=example(2).rb&#34;&gt;&lt;/script&gt;Running the above code will give you the following &lt;i&gt;NoMethodError&lt;/i&gt; message:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6262775.js?file=error&#34;&gt;&lt;/script&gt; The way I went about using complex classes in signatures is to utilise&amp;nbsp;&lt;i&gt;add_method_signature&lt;/i&gt;&amp;nbsp;instead of&amp;nbsp;&lt;i&gt;java_signature&lt;/i&gt;:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6262775.js?file=example(3).rb&#34;&gt;&lt;/script&gt;&lt;i&gt;add_method_signature&lt;/i&gt;&amp;nbsp;expects the first argument to be the name of the method that will have its signature changed. For the second argument, it expects it to be a list of classes. The list&amp;rsquo;s first item is the return class (e.g.,&amp;nbsp;&lt;i&gt;void&lt;/i&gt;) while the subsequent items are the signature&amp;rsquo;s parameter classes (e.g.,&amp;nbsp;&lt;i&gt;int&lt;/i&gt; and &lt;i&gt;MyClass&lt;/i&gt;). Note that I invoke&amp;nbsp;&lt;i&gt;become_java!&lt;/i&gt;&amp;nbsp;on the complex class. This tells &lt;i&gt;MyClass&lt;/i&gt; to materialize itself into a Java class. The &lt;i&gt;false&lt;/i&gt; flag is needed so that JRuby&amp;rsquo;s main class loader is used to load the class. Without it, you&amp;rsquo;ll be greeted by a&amp;nbsp;&lt;i&gt;java.lang.ClassNotFoundException&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>JRuby CXF: A Gem for Creating SOAP Web Services</title>
      <link>http://oncodesign.io/2013/08/10/jruby-cxf-a-gem-for-creating-soap-web-services/</link>
      <pubDate>Sat, 10 Aug 2013 20:07:00 +0000</pubDate>
      
      <guid>http://oncodesign.io/2013/08/10/jruby-cxf-a-gem-for-creating-soap-web-services/</guid>
      <description>&lt;p&gt;It seems to me that &lt;a href=&#34;http://www.w3.org/TR/ws-arch/&#34; target=&#34;_blank&#34;&gt;Web Services&lt;/a&gt; don&amp;rsquo;t receive much love from Rubyists. In fact, of the two Ruby projects I know that add Web Service support (&lt;a href=&#34;http://rubygems.org/gems/soap4r&#34; rel=&#34;&#34; target=&#34;_blank&#34;&gt;SOAP4R&lt;/a&gt; and &lt;a href=&#34;http://rubygems.org/gems/actionwebservice&#34; target=&#34;_blank&#34;&gt;ActionWebService&lt;/a&gt;), both appear to be inactive. Someone might say that if Web Services are a must, then avoid Ruby or put an &lt;a href=&#34;http://www.mulesoft.org/&#34; target=&#34;_blank&#34;&gt;integration layer&lt;/a&gt; between your Ruby application and the client/service. From my experience, life is not always that simple and these solutions might not be applicable.&amp;nbsp; &lt;br /&gt;&lt;br /&gt;The Java ecosystem has a popular and well-supported open source project that is used to build SOAP Web Services and clients. This project is called &lt;a href=&#34;http://cxf.apache.org/&#34; target=&#34;_blank&#34;&gt;Apache CXF&lt;/a&gt;. On one fine sunny day I asked myself: &amp;ldquo;Wouldn&amp;rsquo;t it be great if I could publish a Web Service from Ruby using Apache CXF?&amp;rdquo;. Almost immediately I put that thought away. Trying to integrate a Java library into Ruby is, well, hard in my books. But then&amp;nbsp;&lt;a href=&#34;http://jruby.org/&#34; target=&#34;_blank&#34;&gt;JRuby&lt;/a&gt; popped into my mind. JRuby is the Ruby language implemented in Java. This means that Ruby and Java objects talk to each other with relative ease.&lt;br /&gt;&lt;br /&gt;Seeing the potential in the idea, last week I set about developing a &lt;a href=&#34;https://github.com/claudemamo/jruby-cxf&#34; target=&#34;_blank&#34;&gt;JRuby wrapper gem&lt;/a&gt; for CXF. I must admit it was more challenging than I thought but at the end I was happy with the results. The bulk of the work was customising the &lt;a href=&#34;http://cxf.apache.org/docs/aegis-21.html&#34; target=&#34;_blank&#34;&gt;Aegis data binder&lt;/a&gt; so that it could map &lt;i&gt;&lt;a href=&#34;http://jruby.org/apidocs/org/jruby/RubyObject.html&#34; target=&#34;_blank&#34;&gt;RubyObject&lt;/a&gt;&lt;/i&gt; instances.&lt;br /&gt;&lt;br /&gt;The first step to using the gem is installing it:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6176537.js?file=install.sh&#34;&gt;&lt;/script&gt;A &lt;a href=&#34;https://github.com/claudemamo/jruby-cxf-example&#34; target=&#34;_blank&#34;&gt;code example&lt;/a&gt; is in order here:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6176537.js?file=example(1).rb&#34;&gt;&lt;/script&gt;Publishing the above class as a Web Service means requiring the gem and including the module &lt;i&gt;CXF::WebServiceServlet&lt;/i&gt;:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6176537.js?file=example(2).rb&#34;&gt;&lt;/script&gt;Including &lt;i&gt;WebServiceServlet&lt;/i&gt; causes the class to become a regular Java servlet. This implies that any servlet container can load the Web Service. For this example, I&amp;rsquo;ll load the Web Service using an embedded Jetty:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6176537.js?file=example(3).rb&#34;&gt;&lt;/script&gt;Running the example requires two libraries to be available in the Java classpath: &lt;a href=&#34;http://archive.apache.org/dist/cxf/2.7.6/&#34; target=&#34;_blank&#34;&gt;CXF&amp;nbsp;2.7.6&lt;/a&gt; and &lt;a href=&#34;http://mirror17.pcbsd.org/Mirrors/eclipse/jetty/8.1.12.v20130726/dist/&#34; target=&#34;_blank&#34;&gt;Jetty&amp;nbsp;8&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6176537.js?file=run-example.sh&#34;&gt;&lt;/script&gt;Accessing the URL &lt;i&gt;&lt;a href=&#34;http://localhost:8080/hello-world?wsdl&#34;&gt;http://localhost:8080/hello-world?wsdl&lt;/a&gt;&lt;/i&gt; with a browser will display the following WSDL:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6176537.js?file=hello-world(1).wsdl&#34;&gt;&lt;/script&gt;You&amp;rsquo;ll note that the operations are missing from the WSDL. This is because I didn&amp;rsquo;t tell CXF to expose any of the methods in the class &lt;i&gt;HelloWorld&lt;/i&gt; as Web Service operations. Let me do that now:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6176537.js?file=example(4).rb&#34;&gt;&lt;/script&gt;&lt;i&gt;expose&lt;/i&gt;&amp;nbsp;tells CXF to publish the method denoted by the first argument (i.e., &lt;i&gt;:say_hello&lt;/i&gt;). The second argument in &lt;i&gt;expose&lt;/i&gt; is a map. It should have at a&amp;nbsp;&lt;b&gt;minimum&lt;/b&gt; the following entries:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;&amp;nbsp;&lt;i&gt;expects&lt;/i&gt;&amp;nbsp;- maps to an ordered list of hashes where each hash corresponds to a method parameter and its expected type.&lt;/li&gt;&lt;br /&gt;&lt;li&gt;&amp;nbsp;&lt;i&gt;returns&lt;/i&gt;&amp;nbsp;- maps to the expected return type (e.g., &lt;i&gt;:string&lt;/i&gt;).&lt;/li&gt;&lt;/ol&gt;Re-executing the example will give out this WSDL:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6176537.js?file=hello-world(2).wsdl&#34;&gt;&lt;/script&gt;The gem supports various options to customise the WSDL. For instance, the service name and namespace can be changed:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6176537.js?file=example(5).rb&#34;&gt;&lt;/script&gt;The complete list of options is found in the project repository&amp;rsquo;s&amp;nbsp;&lt;i&gt;&lt;a href=&#34;https://github.com/claudemamo/jruby-cxf/blob/master/README.md&#34; target=&#34;_blank&#34;&gt;README&lt;/a&gt;&lt;/i&gt;&amp;nbsp;file&amp;nbsp;.&lt;br /&gt;&lt;br /&gt;Till now I&amp;rsquo;ve assumed that a Web Service operation will only accept simple types. In the real world we&amp;rsquo;re more likely to be using complex types:&lt;br /&gt;&lt;br /&gt;&lt;script src=&#34;https://gist.github.com/claudemamo/6176537.js?file=example(6).rb&#34;&gt;&lt;/script&gt;I&amp;rsquo;ve added two classes in the example: &lt;i&gt;Animal&lt;/i&gt; and &lt;i&gt;Person&lt;/i&gt;.&amp;nbsp;It is necessary to include the &lt;i&gt;CXF::ComplexType&lt;/i&gt; module so that CXF can derive an XML schema from these classes and embed the schema in the WSDL. A complex type element is declared using the method &lt;i&gt;member&lt;/i&gt;. A&amp;nbsp;&lt;i&gt;member&lt;/i&gt;&amp;nbsp;needs at least a name for the element and its type. You could also declare whether a property is required as seen in the member &lt;i&gt;pet&lt;/i&gt;. The &lt;i&gt;required&lt;/i&gt; option defaults to true if not specified.&lt;br /&gt;&lt;br /&gt;Note that now &lt;i&gt;say_hello&lt;/i&gt;&amp;nbsp;and&amp;nbsp;&lt;i&gt;give_age &lt;/i&gt;are&amp;nbsp;expecting a&amp;nbsp;&lt;i&gt;Person&lt;/i&gt;&amp;nbsp;object instead of primitive types and they are accessing the object via accessors. Behind the scenes the gem creates an accessor for each member that is declared.&lt;br /&gt;&lt;br /&gt;I hope I&amp;rsquo;ve given you enough info to get started out with the gem. My plan is maintain JRuby CXF as I believe it could be useful for those who aren&amp;rsquo;t happy with the current alternatives. Of course, if you find an issue with the gem, I&amp;rsquo;d be more than happy to accept code contributions ;-).&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>